{"random_predictor": {"name": "random_predictor", "data": {"README.md": "# Random Predictor\n\nThis is a random, dummy predictor for use primarily in testing. It literally just spits out random\npredictions. The predictions are not calibrated to the base rate of the task, they are truly just random with\na chance value of 0.5.\n", "model.yaml": {"metadata": {"description": "This is a model that simply generates proabilities randomly from a uniform distribution between 0 and 1 for each sample in the task. It should never be used for anything other than testing or establishing an empirical chance baseline for complex metrics.", "links": [], "contacts": [{"name": "Matthew McDermott", "github_username": "mmcdermott"}]}, "commands": {"unsupervised": null, "supervised": {"train": null, "predict": "python -m MEDS_DEV.models.random_predictor.generate_random_predictions labels_dir={labels_dir} output_dir={output_dir} split={split} dataset_dir={dataset_dir}"}}}}, "children": []}, "cehrbert": {"name": "cehrbert", "data": {"README.md": "# CEHR-BERT Predictor\n\nFor detailed cehr-bert documentation, please visit repo at https://github.com/cumc-dbmi/cehrbert.\n\n> [!WARNING]\n> This model is currently very disk intensive, both in pre-processed datasets and in the huggingface datasets\n> cache. Please ensure you have enough disk space to run this model. Note that you can change the huggingface\n> cache directory by setting the `HF_DATASETS_CACHE` environment variable prior to training this model;\n> setting it to `/dev/null` will _cause the system to error out_ so you should instead set it to a real\n> directory and ensure that disk has sufficient room for the cached files. See\n> https://github.com/Medical-Event-Data-Standard/MEDS-DEV/issues/134 and\n> https://github.com/cumc-dbmi/cehrbert/issues/93 for more information.\n", "model.yaml": {"metadata": {"description": "The CEHR-BERT model. See the referenced paper for full details. This version of the CEHR-BERT model uses default hyperparameters. TODO(@ChaoPang): Add more details here.", "links": ["https://proceedings.mlr.press/v158/pang21a.html"], "contacts": [{"name": "Chao Pang", "github_username": "ChaoPang"}]}, "commands": {"unsupervised": {"train": "python {model_dir}/pretrain_cehrbert.py output_dir={output_dir} dataset_dir={dataset_dir} demo={demo}"}, "supervised": {"train": "python {model_dir}/finetune_cehrbert.py output_dir={output_dir} model_initialization_dir={model_initialization_dir} dataset_dir={dataset_dir} labels_dir={labels_dir} demo={demo}", "predict": "python {model_dir}/generate_cehrbert_predictions.py labels_dir={labels_dir} output_dir={output_dir} split={split} dataset_dir={dataset_dir} model_initialization_dir={model_initialization_dir}"}}}, "refs.bib": "@inproceedings{pang2021cehr,\n  title={CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks},\n  author={Pang, Chao and Jiang, Xinzhuo and Kalluri, Krishna S and Spotnitz, Matthew and Chen, RuiJun and Perotte, Adler and Natarajan, Karthik},\n  booktitle={Machine Learning for Health},\n  pages={239--260},\n  year={2021},\n  organization={PMLR}\n}\n", "requirements.txt": ["cehrbert==1.3.5", "meds_reader==0.1.9", "meds-evaluation==0.0.2", "pyyaml", "hydra-core", "polars"]}, "children": []}, "meds_tab/tiny": {"name": "meds_tab/tiny", "data": {"README.md": "# MEDS-Tab Model\n\nThis is a tiny model that uses the MEDS-Tab library to tabularize data and then uses an xgboost classifier to make\npredictions. Only the top 100 most prevalent codes are used, and time windows of 7 days and 30 days are used. Three aggregation methods are used in tabularization: checking for static presence of values, counting code occurrences, and summing numeric values.\n", "model.yaml": {"metadata": {"description": "A tiny, low-cost XGBoost baseline using the MEDS-Tab framework. TODO(@Oufattole): Add more details here.", "links": ["https://arxiv.org/abs/2411.00200"], "contacts": [{"name": "Nassim Oufattole", "github_username": "Oufattole"}, {"name": "Teya Bergamaschi", "github_username": "teyaberg"}, {"name": "Matthew McDermott", "github_username": "mmcdermott"}]}, "commands": {"unsupervised": null, "supervised": {"train": "meds-tab-describe \\\n    \"input_dir={dataset_dir}/data\" \"output_dir={output_dir}/meds_tab\"\n\nmeds-tab-tabularize-static \\\n    \"input_dir={dataset_dir}/data\" \"output_dir={output_dir}/meds_tab\" \\\n    do_overwrite=False \"input_label_dir={labels_dir}\" \\\n    \"tabularization.aggs=[code/count,value/sum]\" \\\n    \"tabularization.window_sizes=[7d,30d]\" \\\n    \"tabularization.max_included_codes=100\"\n\nmeds-tab-tabularize-time-series \\\n    --multirun \\\n    worker=\"range(0,1)\" \\\n    hydra/launcher=joblib \\\n    \"input_dir={dataset_dir}/data\" \"output_dir={output_dir}/meds_tab\" \\\n    do_overwrite=False \"input_label_dir={labels_dir}\" \\\n    \"tabularization.aggs=[code/count,value/sum]\" \\\n    \"tabularization.window_sizes=[7d,30d]\" \\\n    \"tabularization.max_included_codes=100\"\n\nmeds-tab-xgboost \\\n    --multirun \\\n    \"input_dir={dataset_dir}/data\" \"output_dir={output_dir}/meds_tab\" \\\n    \"output_model_dir={output_dir}/results\" \"task_name=null\" do_overwrite=False \\\n    \"hydra.sweeper.n_trials=20\" \"hydra.sweeper.n_jobs=1\" \\\n    \"hydra.sweeper.max_failure_rate=0.5\" \\\n    \"input_tabularized_cache_dir={output_dir}/meds_tab/tabularize/\" \\\n    \"input_label_cache_dir={labels_dir}\" \\\n    \"tabularization.aggs=[code/count,value/sum]\" \\\n    \"tabularization.window_sizes=[7d,30d]\" \\\n    \"tabularization.max_included_codes=100\" tabularization.min_code_inclusion_count=null", "predict": "mkdir -p \"{output_dir}\"\nLATEST_TRAIN_DIR=$(ls -td {output_dir}/../train/results/*/ | head -n 1)\ncp $LATEST_TRAIN_DIR/best_trial/{split}_predictions.parquet {output_dir}/predictions.parquet"}}}, "requirements.txt": ["meds-tab==0.1", "hydra-optuna-sweeper==1.3.0.dev0"]}, "children": []}, "meds_tab": {"name": "meds_tab", "data": {"README.md": "# MEDS-Tab Model\n\nMEDS-Tab is a tool for tabularizing and modeling complex medical time-series data. It works by:\n\n- Leveraging sparse representations during tabularization\n- Utilizing parallelism over shards\n- Computing aggregations over various time windows and codes\n\nThis approach significantly reduces the computation required to generate high-quality tree-based baseline models for any supervised learning task, while leveraging as many features as possible.\n\n## Feature Generation\n\nMEDS-Tab generates features by computing the cross product of three elements:\n\n1. A MEDS `code`\n2. User-defined time windows (e.g., 1 day, 1 year, full patient history)\n3. The following aggregation functions for the subset of observations that fall within the time window:\n\n- `code/count`: Number of times a `code` was observed\n- `value/count`: Number of times a `code` had a `numeric_value`\n- `value/sum`: Sum of `numeric_value` for a `code`\n- `value/sum_sqd`: Sum of squared `numeric_value` for a `code`\n- `value/min`: Minimum `numeric_value` for a `code`\n- `value/max`: Maximum `numeric_value` for a `code`\n- `static/present`: Binary indicator for presence of a `code` with null `time`\n- `static/first`: `numeric_value` of a `code` with null `time`\n\n## Citation\n\nIf you use MEDS-Tab in your research, please cite:\n\n```bibtex\n@misc{oufattole2024medstabautomatedtabularizationbaseline,\n      title={MEDS-Tab: Automated tabularization and baseline methods for MEDS datasets},\n      author={Nassim Oufattole and Teya Bergamaschi and Aleksia Kolo and Hyewon Jeong and Hanna Gaggin and Collin M. Stultz and Matthew B. A. McDermott},\n      year={2024},\n      eprint={2411.00200},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2411.00200},\n}\n```\n", "refs.bib": "@misc{oufattole2024medstabautomatedtabularizationbaseline,\n      title={MEDS-Tab: Automated tabularization and baseline methods for MEDS datasets},\n      author={Nassim Oufattole and Teya Bergamaschi and Aleksia Kolo and Hyewon Jeong and Hanna Gaggin and Collin M. Stultz and Matthew B. A. McDermott},\n      year={2024},\n      eprint={2411.00200},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2411.00200},\n}\n"}, "children": ["meds_tab/tiny"]}, "genhpf": {"name": "genhpf", "data": {"README.md": "# GenHPF Predictor for MEDS dataset\n\nFor detailed documentation for GenHPF model, please refer to [https://github.com/hoon9405/GenHPF](https://github.com/hoon9405/GenHPF).\n", "model.yaml": {"metadata": {"description": "The GenHPF model. See the referenced paper for full details. This version of the GenHPF model uses default hyperparameters. Note that we need to preprocess MEDS dataset to match the input format for the GenHPF model, `train` mode involves the preprocessing step before training the model, which makes the `train` step take quite long time for the first run. The processed data will be save in the `{output_dir}/data` directory, and the trained model checkpoint will be saved in the `{output_dir}/checkpoints` directory. The `predict` mode will load the trained checkpoint by `{output_dir}/checkpoints/checkpoint_best.pt` and output predictions for the test subset (specified by `split`; e.g., `held_out`) in the `{output_dir}/predictions.parquet` file, which should be compatible with the meds-evaluation pipeline.", "links": ["https://arxiv.org/abs/2207.09858"], "contacts": [{"name": "Jungwoo Oh", "github_username": "jwoo5"}]}, "commands": {"unsupervised": null, "supervised": {"train": "genhpf-preprocess-meds \\\n  \"{dataset_dir}/data\" \\\n  \"--cohort={labels_dir}\" \\\n  \"--metadata_dir={dataset_dir}/metadata\" \\\n  \"--output_dir={output_dir}/data\" \\\n  \"--workers=32\" \\\n  \"--debug={demo}\" \\\n  \"--skip-if-exists\"\n\ngenhpf-train \\\n  \"hydra.run.dir={output_dir}\" \\\n  \"dataset.data={output_dir}/data\" \\\n  \"checkpoint.save_dir={output_dir}/checkpoints\" \\\n  \"common.debug={demo}\" \\\n  \"--config-dir={model_dir}\" \\\n  \"--config-name=genhpf_train\"", "predict": "genhpf-test \\\n  \"hydra.run.dir={output_dir}\" \\\n  \"dataset.data={model_initialization_dir}/data\" \\\n  \"meds.output_predictions=true\" \\\n  \"meds.labels_dir={labels_dir}\" \\\n  \"meds.output_dir={output_dir}\" \\\n  \"checkpoint.load_checkpoint={model_initialization_dir}/checkpoints/checkpoint_best.pt\" \\\n  \"dataset.test_subset={split}\" \\\n  \"common.debug={demo}\" \\\n  \"--config-dir={model_dir}\" \\\n  \"--config-name=genhpf_predict\"\n\nmv {output_dir}/{split}.parquet {output_dir}/predictions.parquet"}}}, "refs.bib": "@article{hur2023genhpf,\n  title={GenHPF: General Healthcare Predictive Framework for Multi-task Multi-source Learning},\n  author={Hur, Kyunghoon and Oh, Jungwoo and Kim, Junu and Kim, Jiyoun and Lee, Min Jae and Cho, Eunbyeol and Moon, Seong-Eun and Kim, Young-Hak and Atallah, Louis and Choi, Edward},\n  journal={IEEE Journal of Biomedical and Health Informatics},\n  year={2023},\n  publisher={IEEE}\n}\n", "requirements.txt": ["genhpf==1.0.10", "pyarrow==17.0.0"]}, "children": []}}